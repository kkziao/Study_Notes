{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnokyRPqBJ7n"
   },
   "source": [
    "# **Logistic Regression Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JXCMO-KSHept"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-m0iExC9bm4"
   },
   "source": [
    "# **1. Logistic Regression [23pts]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9uBsNwS9c32"
   },
   "source": [
    "## **1.1. Logistic Regression Implementation [18 pts, autograded]**\n",
    "\n",
    "Implement logistic regression with both L1 and L2 regularization by completing the LogisticRegression class.  \n",
    "\n",
    "Your class must implement the following API:\n",
    "\n",
    "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
    "* `sigmoid(x)`\n",
    "* `compute_cost(theta, X, y)`\n",
    "* `compute_gradient(theta, X, y)`\n",
    "* `has_converged(theta_old, theta_new)`\n",
    "* `fit(X, y)`\n",
    "* `predict_proba(X)`\n",
    "* `predict(X)`\n",
    "\n",
    "Note that these methods have already been defined correctly for you in the LogisticRegression class. **DO NOT** change the API.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1.1. Sigmoid Function [1 pt]**\n",
    "\n",
    "You should begin by implementing the `sigmoid` function.  As you may know, the sigmoid function $\\sigma(x)$ is mathematically defined as follows.\n",
    "\n",
    "> $\\sigma(x) = \\frac{1}{1\\ +\\ \\text{exp}(-x)}$\n",
    "\n",
    "**Be certain that your sigmoid function works with both vectors and matrices** --- for either a vector or a matrix, you function should perform the sigmoid function on every element.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1.2. Cost Function [5 pts]**\n",
    "\n",
    "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
    "\n",
    "> $\n",
    "\\mathcal{L}({\\theta}) = -\\sum_{i =1}^N [ y_i\\log(h_{{\\theta}}({x}_i)) + (1 - y_i)\\log(1 - h_{{\\theta}}({x}_i))]\n",
    "$\n",
    "\n",
    "where\n",
    "> $\n",
    "h_{\\theta}(x_{i}) = \\sigma(\\theta^{T}x_{i})\n",
    "$\n",
    "\n",
    "\n",
    "L1 Regularisation Loss:\n",
    ">$\n",
    "\\mathcal{L1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  |{\\theta}_j|\n",
    "$\n",
    "\n",
    "L2 Regularisation Loss:\n",
    ">$\n",
    "\\mathcal{L2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  {\\theta}_j^2\n",
    "$\n",
    "\n",
    "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1.3. Gradient of the Cost Function [5 pts]**\n",
    "\n",
    "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1.4. Convergence Check [1 pt]**\n",
    "\n",
    "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer 2.1.5 for convergence condition.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1.5. Training [3 pts]**\n",
    "\n",
    "The `fit` method should train the model via gradient descent, relying on the cost and gradient functions. The trained weights/coefficients must be stored as `theta_`. The weights start as a zero vector. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
    "\n",
    "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when\n",
    "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$,\n",
    "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance).\n",
    "\n",
    "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
    "\n",
    "* `alpha` is the learning rate of the gradient descent algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1.6. Predict Probability [1 pt]**\n",
    "\n",
    "The `predict_probability` function should predict the probabilities that the data points in a given input data matrix belong to class 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1.7. Predict [2 pts]**\n",
    "\n",
    "The `predict` function should predict the classes of the data points in a given input data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TVWOemIR8-09"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    \"\"\"\n",
    "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: float, default=0.01\n",
    "        Learning rate\n",
    "    tol : float, default=0.0001\n",
    "        Tolerance for stopping criteria\n",
    "    max_iter : int, default=10000\n",
    "        Maximum number of iterations of gradient descent\n",
    "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
    "        The initial weights; if None, all weights will be zero by default\n",
    "    penalty : string, default = None\n",
    "        The type of regularization. The other acceptable options are l1 and l2\n",
    "    lambd : float, default = 1.0\n",
    "        The parameter regularisation constant (i.e. lambda)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    theta_ : numpy.ndarray of shape (D + 1,)\n",
    "        The value of the coefficients after gradient descent has converged\n",
    "        or the number of iterations hit the maximum limit\n",
    "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
    "        Stores theta_ after every gradient descent iteration\n",
    "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
    "        Stores cost after every gradient descent iteration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.01, tol=0.0001, max_iter=10000, theta_init=None, penalty = None, lambd = 1.0):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.theta_init = theta_init\n",
    "        self.penalty = penalty\n",
    "        self.lambd = lambd\n",
    "        self.theta_ = None\n",
    "        self.hist_cost_ = None\n",
    "        self.hist_theta_ = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # a function needed for using cross_val_score function from sklearn.model_selection\n",
    "        return {\"alpha\": self.alpha, \"max_iter\": self.max_iter, \"lambd\" : self.lambd, \"penalty\" : self.penalty}\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the sigmoid value of the argument.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: numpy.ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out: numpy.ndarray\n",
    "            The sigmoid value of x\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        # TODO END\n",
    "\n",
    "    def compute_cost(self, theta, X, y):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the cost/objective function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta: numpy.ndarray of shape (D + 1,)\n",
    "            The coefficients\n",
    "        X: numpy.ndarray of shape (N, D + 1)\n",
    "            The features matrix\n",
    "        y: numpy.ndarray of shape (N,)\n",
    "            The target variable array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cost: float\n",
    "            The cost as a scalar value\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
    "        # DO NOT use np.dot for this function as it can possibly return nan. Use a combination of np.nansum and np.multiply.\n",
    "\n",
    "        z = np.nansum(np.multiply(X, theta), axis=1)\n",
    "        h = self.sigmoid(z)\n",
    "\n",
    "        cost = -np.nansum(np.multiply(y, np.log(h)) + np.multiply((1 - y), np.log(1 - h)))\n",
    "        # CASE 1: NO PENALTY\n",
    "        if self.penalty is None:\n",
    "            return cost\n",
    "\n",
    "        # CASE 2: L1 PENALTY\n",
    "        elif self.penalty == 'l1':\n",
    "            l1_penalty = self.lambd * np.nansum(np.abs(theta[1:]))\n",
    "            return cost + l1_penalty\n",
    "\n",
    "        # CASE 3: L2 PENALTY\n",
    "        elif self.penalty == 'l2':\n",
    "            l2_penalty = self.lambd * np.nansum(np.square(theta[1:]))\n",
    "            return cost + l2_penalty\n",
    "\n",
    "        # TODO END\n",
    "\n",
    "    def compute_gradient(self, theta, X, y):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the gradient of the cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta: numpy.ndarray of shape (D + 1,)\n",
    "            The coefficients\n",
    "        X: numpy.ndarray of shape (N, D + 1)\n",
    "            The features matrix\n",
    "        y: numpy.ndarray of shape (N,)\n",
    "            The target variable array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient: numpy.ndarray of shape (D + 1,)\n",
    "            The gradient values\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
    "        z = np.nansum(np.multiply(X, theta), axis=1)\n",
    "        h = self.sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h - y))\n",
    "\n",
    "        # L1 Regularization\n",
    "        if self.penalty == 'l1':\n",
    "            gradient[1:] += self.lambd * np.sign(theta[1:])\n",
    "\n",
    "        # L2 Regularization\n",
    "        elif self.penalty == 'l2':\n",
    "            gradient[1:] += 2 * self.lambd * theta[1:]\n",
    "\n",
    "        return gradient\n",
    "        # TODO END\n",
    "\n",
    "    def has_converged(self, theta_old, theta_new):\n",
    "\n",
    "        \"\"\"\n",
    "        Return whether gradient descent has converged.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta_old: numpy.ndarray of shape (D + 1,)\n",
    "            The weights prior to the update by gradient descent\n",
    "        theta_new: numpy.ndarray of shape (D + 1,)\n",
    "            The weights after the update by gradient descent\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        converged: bool\n",
    "            Whether gradient descent converged or not\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "        difference = np.linalg.norm(theta_new - theta_old, ord=2)\n",
    "        return difference < self.tol\n",
    "        # TODO END\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the coefficients using gradient descent and store them as theta_.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray of shape (N, D)\n",
    "            The features matrix\n",
    "        y: numpy.ndarray of shape (N,)\n",
    "            The target variable array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing\n",
    "        \"\"\"\n",
    "\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Adding a column of ones at the beginning for the bias term\n",
    "        ones_col = np.ones((N, 1))\n",
    "        X = np.hstack((ones_col, X))\n",
    "\n",
    "        # Initializing the weights\n",
    "        if self.theta_init is None:\n",
    "            theta_old = np.zeros((D + 1,))\n",
    "        else:\n",
    "            theta_old = self.theta_init\n",
    "\n",
    "        # Initializing the historical weights matrix\n",
    "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
    "        self.hist_theta_ = np.array([theta_old])\n",
    "\n",
    "        # Computing the cost for the initial weights\n",
    "        cost = self.compute_cost(theta_old, X, y)\n",
    "\n",
    "        # Initializing the historical cost array\n",
    "        # Remember to append this array with the cost after every gradient descent iteration\n",
    "        self.hist_cost_ = np.array([cost])\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            # Gradient Calculation\n",
    "            gradient = self.compute_gradient(theta_old, X, y)\n",
    "\n",
    "            theta_new = theta_old - self.alpha * gradient\n",
    "\n",
    "            self.hist_theta_ = np.vstack([self.hist_theta_, theta_new])\n",
    "            cost = self.compute_cost(theta_new, X, y)\n",
    "            self.hist_cost_ = np.append(self.hist_cost_, cost)\n",
    "\n",
    "            # Check for convergence\n",
    "            if self.has_converged(theta_old, theta_new):\n",
    "                break\n",
    "\n",
    "            # Update theta_old for the next iteration\n",
    "            theta_old = theta_new\n",
    "\n",
    "        self.theta_ = theta_new\n",
    "        # TODO END\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "        Predict the probabilities that the data points in X belong to class 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray of shape (N, D)\n",
    "            The features matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_hat: numpy.ndarray of shape (N,)\n",
    "            The predicted probabilities that the data points in X belong to class 1\n",
    "        \"\"\"\n",
    "\n",
    "        N = X.shape[0]\n",
    "        X = np.hstack((np.ones((N, 1)), X))\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "        z = np.dot(X, self.theta_)\n",
    "        y_hat = self.sigmoid(z)\n",
    "\n",
    "        return y_hat\n",
    "        # TODO END\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "        Predict the classes of the data points in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray of shape (N, D)\n",
    "            The features matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: numpy.ndarray of shape (N,)\n",
    "            The predicted class of the data points in X\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "        y_proba = self.predict_proba(X)\n",
    "\n",
    "        y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "        return y_pred\n",
    "        # TODO END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4edZSvLt8_Km"
   },
   "outputs": [],
   "source": [
    "def test_log_reg_sigmoid(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression()\n",
    "    test_case = np.array([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n",
    "    student_ans = student_lr_clf.sigmoid(test_case)\n",
    "    required_ans = np.array([0.83539354, 0.35165864, 0.3709434 , 0.25483894, 0.70378922])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_sigmoid(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p37bE3an8_QK"
   },
   "outputs": [],
   "source": [
    "def test_log_reg_compute_cost(StudentLogisticRegression):\n",
    "\n",
    "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
    "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
    "                            [-0.52817175, -1.07296862],\n",
    "                            [ 0.86540763, -2.3015387 ],\n",
    "                            [ 1.74481176, -0.7612069 ],\n",
    "                            [ 0.3190391,  -0.24937038]])\n",
    "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression()\n",
    "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
    "\n",
    "    required_ans = 7.467975765663204\n",
    "\n",
    "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
    "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = 7.52915138076548\n",
    "\n",
    "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
    "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = 7.505400330283089\n",
    "\n",
    "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_compute_cost(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SI-neAfu8_Ub"
   },
   "outputs": [],
   "source": [
    "def test_log_reg_compute_gradient(StudentLogisticRegression):\n",
    "\n",
    "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
    "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
    "                            [-0.52817175, -1.07296862],\n",
    "                            [ 0.86540763, -2.3015387 ],\n",
    "                            [ 1.74481176, -0.7612069 ],\n",
    "                            [ 0.3190391,  -0.24937038]])\n",
    "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression()\n",
    "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = np.array([ 2.60573737, -2.20203139])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
    "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = np.array([ 2.60573737, -2.30203139])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
    "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = np.array([ 2.60573737, -2.32438267])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_compute_gradient(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mhQ0YqCU-SeO"
   },
   "outputs": [],
   "source": [
    "def test_log_reg_has_converged(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression()\n",
    "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
    "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
    "    student_ans = student_lr_clf.has_converged(test_case_theta_old, test_case_theta_new)\n",
    "    required_ans = True\n",
    "\n",
    "    assert student_ans == required_ans\n",
    "\n",
    "test_log_reg_has_converged(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QJC7W7Lh-SjY"
   },
   "outputs": [],
   "source": [
    "def test_log_reg_fit(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
    "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
    "                            [-0.52817175, -1.07296862],\n",
    "                            [ 0.86540763, -2.3015387 ],\n",
    "                            [ 1.74481176, -0.7612069 ],\n",
    "                            [ 0.3190391,  -0.24937038]])\n",
    "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
    "    student_lr_clf.fit(test_case_X, test_case_y)\n",
    "    student_ans = student_lr_clf.hist_theta_\n",
    "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
    "                             [ 0.005     , -0.00597503,  0.00564325],\n",
    "                             [ 0.01006813, -0.01184464,  0.0111865 ],\n",
    "                             [ 0.01520121, -0.01761226,  0.01663348],\n",
    "                             [ 0.02039621, -0.02328121,  0.02198778],\n",
    "                             [ 0.02565018, -0.0288547 ,  0.02725288]])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_fit(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "toqYtIpX-Snf"
   },
   "outputs": [],
   "source": [
    "def test_log_reg_predict_proba(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
    "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
    "                            [-0.52817175, -1.07296862],\n",
    "                            [ 0.86540763, -2.3015387 ],\n",
    "                            [ 1.74481176, -0.7612069 ],\n",
    "                            [ 0.3190391,  -0.24937038]])\n",
    "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
    "    student_lr_clf.fit(test_case_X, test_case_y)\n",
    "    student_ans = student_lr_clf.predict_proba(test_case_X)\n",
    "    required_ans = np.array([0.49052814, 0.5029122 , 0.48449386, 0.48864172, 0.50241207])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_predict_proba(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M6yb0tUR-alL"
   },
   "outputs": [],
   "source": [
    "def test_log_reg_predict(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
    "    np.random.seed(1)\n",
    "    test_case_X = np.random.randn(50, 2)\n",
    "    test_case_y = np.random.randint(0, 2, 50)\n",
    "    student_lr_clf.fit(test_case_X, test_case_y)\n",
    "    student_ans = student_lr_clf.predict(test_case_X)\n",
    "    required_ans = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
    "                             0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])\n",
    "\n",
    "    assert np.mean(np.abs(student_ans - required_ans)) <= 0.02\n",
    "\n",
    "test_log_reg_predict(LogisticRegression)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
